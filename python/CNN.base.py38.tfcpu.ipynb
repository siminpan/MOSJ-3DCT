{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.measure\n",
    "import pydicom\n",
    "import vtk\n",
    "from vtk.util import numpy_support\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "os.chdir('C:/Users/span/Documents/3DSlicerTutorial/CNN.test/')\n",
    "number1 = \"23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathDicom = \"./\"+str(number1)+\"_dl_input/\"\n",
    "reader = vtk.vtkDICOMImageReader()\n",
    "reader.SetDirectoryName(PathDicom)\n",
    "reader.Update()\n",
    "\n",
    "# Load dimensions using `GetDataExtent`\n",
    "_extent = reader.GetDataExtent()\n",
    "ConstPixelDims = [_extent[1]-_extent[0]+1, _extent[3]-_extent[2]+1, _extent[5]-_extent[4]+1]\n",
    "\n",
    "# Load spacing values\n",
    "ConstPixelSpacing = reader.GetPixelSpacing()\n",
    "\n",
    "# Get the 'vtkImageData' object from the reader\n",
    "imageData = reader.GetOutput()\n",
    "# Get the 'vtkPointData' object from the 'vtkImageData' object\n",
    "pointData = imageData.GetPointData()\n",
    "# Ensure that only one array exists within the 'vtkPointData' object\n",
    "assert (pointData.GetNumberOfArrays()==1)\n",
    "# Get the `vtkArray` (or whatever derived type) which is needed for the `numpy_support.vtk_to_numpy` function\n",
    "arrayData = pointData.GetArray(0)\n",
    "\n",
    "# Convert the `vtkArray` to a NumPy array\n",
    "ArrayDicom = numpy_support.vtk_to_numpy(arrayData)\n",
    "# Reshape the NumPy array to 3D using 'ConstPixelDims' as a 'shape'\n",
    "ArrayDicom = ArrayDicom.reshape(ConstPixelDims, order='F')\n",
    "\n",
    "a_in = ArrayDicom.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathDicom = \"./\"+str(number1)+\"_dl_mask/\"\n",
    "reader = vtk.vtkDICOMImageReader()\n",
    "reader.SetDirectoryName(PathDicom)\n",
    "reader.Update()\n",
    "\n",
    "# Load dimensions using `GetDataExtent`\n",
    "_extent = reader.GetDataExtent()\n",
    "ConstPixelDims = [_extent[1]-_extent[0]+1, _extent[3]-_extent[2]+1, _extent[5]-_extent[4]+1]\n",
    "\n",
    "# Load spacing values\n",
    "ConstPixelSpacing = reader.GetPixelSpacing()\n",
    "\n",
    "# Get the 'vtkImageData' object from the reader\n",
    "imageData = reader.GetOutput()\n",
    "# Get the 'vtkPointData' object from the 'vtkImageData' object\n",
    "pointData = imageData.GetPointData()\n",
    "# Ensure that only one array exists within the 'vtkPointData' object\n",
    "assert (pointData.GetNumberOfArrays()==1)\n",
    "# Get the `vtkArray` (or whatever derived type) which is needed for the `numpy_support.vtk_to_numpy` function\n",
    "arrayData = pointData.GetArray(0)\n",
    "\n",
    "# Convert the `vtkArray` to a NumPy array\n",
    "ArrayDicom = numpy_support.vtk_to_numpy(arrayData)\n",
    "# Reshape the NumPy array to 3D using 'ConstPixelDims' as a 'shape'\n",
    "ArrayDicom = ArrayDicom.reshape(ConstPixelDims, order='F')\n",
    "\n",
    "a_mask = ArrayDicom.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert a_in.shape == a_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(a=1024, version=2)\n",
    "list1 = list(range(a_in.shape[2]))\n",
    "trainlist1 = random.sample(list1, k=round(a_in.shape[2]*0.6))\n",
    "testlist1 = [x for x in list1 if (x not in trainlist1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = a_in[:, :, i].astype(np.uint8).copy()\n",
    "image1 = rescale(image, (512/a_in[:, :, i].shape[1]), anti_aliasing=False)\n",
    "image2 = (((image1 - image1.min()) / (image1.max() - image1.min())) * 255.9).astype(np.uint8)\n",
    "img0 = Image.fromarray(image2.astype(np.uint8), mode = \"L\")\n",
    "plt.imshow(img0)\n",
    "image3 = image2[..., np.newaxis]\n",
    "# img0.save('./data/train_image/23_train_image/im0.jpg')\n",
    "plt.imsave('./data/train_image/23_train_image/im0.png', image2, format='png', cmap='gray')\n",
    "# cv2.imwrite('./data/train_image/23_train_image/img-png-gray.jpg', image3[:, :,0].astype(np.uint8)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img22 = cv2.imread('./data/train_image/23_train_image/im0.png')\n",
    "img22.shape\n",
    "# img23 = cv2.imread('./data/train_image/23_train_image/img-png-gray.jpg')\n",
    "# print(img23.shape,img22.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-e8bcdc10030f>:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  image = (((image - image.min()) / (image.max() - image.min())) * 255.9).astype(np.uint8)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from skimage.transform import rescale\n",
    "# output  23_train_image\n",
    "path1 = \"data/train_image/23_train_image/train/\"\n",
    "path2 = \"data/train_mask/23_train_mask/train/\"\n",
    "path3 = \"data/val_image/23_test_image/val/\"\n",
    "path4 = \"data/val_mask/23_test_mask/val/\"\n",
    "\n",
    "resize1 = 256\n",
    "\n",
    "# output  23_train_image\n",
    "for i in trainlist1:\n",
    "    image = a_in[:, :, i].astype(np.uint8).copy()\n",
    "    image = rescale(image, (resize1/a_in[:, :, i].shape[1]), anti_aliasing=False)\n",
    "    image = (((image - image.min()) / (image.max() - image.min())) * 255.9).astype(np.uint8)\n",
    "    img0 = Image.fromarray(image)\n",
    "    img0.save(path1+str(number1)+\"_taimage_\"+str(\"{0:03}\".format(i))+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-76bb4e6b7e79>:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  image = (((image - image.min()) / (image.max() - image.min())) * 255.9).astype(np.uint8)\n",
      "<ipython-input-10-76bb4e6b7e79>:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  image = (((image - image.min()) / (image.max() - image.min())) * 255.9).astype(np.uint8)\n",
      "<ipython-input-10-76bb4e6b7e79>:23: RuntimeWarning: invalid value encountered in true_divide\n",
      "  image = (((image - image.min()) / (image.max() - image.min())) * 255.9).astype(np.uint8)\n"
     ]
    }
   ],
   "source": [
    "# output  23_train_mask\n",
    "\n",
    "for i in trainlist1:\n",
    "    image = a_mask[:, :, i].astype(np.uint8).copy()\n",
    "    image = rescale(image, (resize1/a_in[:, :, i].shape[1]), anti_aliasing=False)\n",
    "    image = (((image - image.min()) / (image.max() - image.min())) * 255.9).astype(np.uint8)\n",
    "    img0 = Image.fromarray(image)\n",
    "    img0.save(path2+str(number1)+\"_tamask_\"+str(\"{0:03}\".format(i))+\".png\")\n",
    "\n",
    "# output  23_train_image\n",
    "\n",
    "for i in testlist1:\n",
    "    image = a_in[:, :, i].astype(np.uint8).copy()\n",
    "    image = rescale(image, (resize1/a_in[:, :, i].shape[1]), anti_aliasing=False)\n",
    "    image = (((image - image.min()) / (image.max() - image.min())) * 255.9).astype(np.uint8)\n",
    "    img0 = Image.fromarray(image)\n",
    "    img0.save(path3+str(number1)+\"_tsimage_\"+str(\"{0:03}\".format(i))+\".png\")\n",
    "# output  23_train_mask\n",
    "\n",
    "for i in testlist1:\n",
    "    image = a_mask[:, :, i].astype(np.uint8).copy()\n",
    "    image = rescale(image, (resize1/a_in[:, :, i].shape[1]), anti_aliasing=False)\n",
    "    image = (((image - image.min()) / (image.max() - image.min())) * 255.9).astype(np.uint8)\n",
    "    img0 = Image.fromarray(image)\n",
    "    img0.save(path4+str(number1)+\"_tsmask_\"+str(\"{0:03}\".format(i))+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = 25\n",
    "i2 = 45\n",
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(1, 4, figsize=(16, 6.4),\n",
    "                                    sharex=True, sharey=True)\n",
    "ax0.imshow(a_in[:, :, trainlist1[i1]], cmap='gray')\n",
    "ax0.axis('off')\n",
    "ax0.set_title('a in 1')\n",
    "ax1.imshow(a_mask[:, :, trainlist1[i1]], cmap='gray')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('a out 1')\n",
    "ax2.imshow(a_in[:, :, trainlist1[i2]], cmap='gray') # 'magma'\n",
    "ax2.axis('off')\n",
    "ax2.set_title('a in 2')\n",
    "ax3.imshow(a_mask[:, :, trainlist1[i2]], cmap='gray')\n",
    "ax3.axis('off')\n",
    "ax3.set_title('a out 2')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True\n",
    ")\n",
    "        \n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 337 images belonging to 1 classes.\n",
      "Found 337 images belonging to 1 classes.\n",
      "Found 225 images belonging to 1 classes.\n",
      "Found 225 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "path1 = \"data/train_image/23_train_image\"\n",
    "path2 = \"data/train_mask/23_train_mask\"\n",
    "path3 = \"data/val_image/23_test_image\"\n",
    "path4 = \"data/val_mask/23_test_mask\"\n",
    "\n",
    "train_image_generator = train_datagen.flow_from_directory(\n",
    "    path1,\n",
    "    color_mode = \"grayscale\",\n",
    "    batch_size = 4 #NORMALLY 4/8/16/32\n",
    ")\n",
    "\n",
    "train_mask_generator = train_datagen.flow_from_directory(\n",
    "    path2,\n",
    "    color_mode = \"grayscale\",\n",
    "    batch_size = 4 #NORMALLY 4/8/16/32\n",
    ")\n",
    "\n",
    "val_image_generator = val_datagen.flow_from_directory(\n",
    "    path3,\n",
    "    color_mode = \"grayscale\",\n",
    "    batch_size = 4 #NORMALLY 4/8/16/32\n",
    ")\n",
    "\n",
    "\n",
    "val_mask_generator = val_datagen.flow_from_directory(\n",
    "    path4,\n",
    "    color_mode = \"grayscale\",\n",
    "    batch_size = 4 #NORMALLY 4/8/16/32\n",
    ")\n",
    "\n",
    "train_generator = zip(train_image_generator, train_mask_generator)\n",
    "val_generator = zip(val_image_generator, val_mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_model(optimizer, loss_metric, metrics, lr=1e-3):\n",
    "    inputs = keras.Input((sample_width, sample_height, 1))\n",
    "    conv1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='SAME')(inputs)\n",
    "    conv1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='SAME')(conv1)\n",
    "    conv1 = tf.reshape(conv1, tf.shape(conv1))\n",
    "    pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    drop1 = keras.layers.Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='SAME')(drop1)\n",
    "    conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='SAME')(conv2)\n",
    "    conv2 = tf.reshape(conv2, tf.shape(conv2))\n",
    "    pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    drop2 = keras.layers.Dropout(0.5)(pool2)\n",
    "\n",
    "    conv3 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='SAME')(drop2)\n",
    "    conv3 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='SAME')(conv3)\n",
    "    conv3 = tf.reshape(conv3, tf.shape(conv3))\n",
    "    pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    drop3 = keras.layers.Dropout(0.3)(pool3)\n",
    "\n",
    "    conv4 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='SAME')(drop3)\n",
    "    conv4 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='SAME')(conv4)\n",
    "    conv4 = tf.reshape(conv4, tf.shape(conv4))\n",
    "    pool4 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    drop4 = keras.layers.Dropout(0.3)(pool4)\n",
    "\n",
    "    conv5 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='SAME')(drop4)\n",
    "    conv5 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='SAME')(conv5)\n",
    "    conv5 = tf.reshape(conv5, tf.shape(conv5))\n",
    "    \n",
    "    conv5u = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=(2, 2), strides=(2, 2), padding='SAME')(conv5)\n",
    "    conv5u = tf.reshape(conv5u, tf.shape(conv5u))\n",
    "    up6 = keras.layers.concatenate([conv5u, conv4], axis=3)\n",
    "    conv6 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='SAME')(up6)\n",
    "    conv6 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='SAME')(conv6)\n",
    "    conv6 = tf.reshape(conv6, tf.shape(conv6))\n",
    "    \n",
    "    conv6u = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=(2, 2), strides=(2, 2), padding='SAME')(conv6)\n",
    "    conv6u = tf.reshape(conv6u, tf.shape(conv6u))\n",
    "    up7 = keras.layers.concatenate([conv6u, conv3], axis=3)\n",
    "    conv7 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='SAME')(up7)\n",
    "    conv7 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='SAME')(conv7)\n",
    "    conv7 = tf.reshape(conv7, tf.shape(conv7))\n",
    "    \n",
    "    conv7u = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(2, 2), strides=(2, 2), padding='SAME')(conv7)\n",
    "    conv7u = tf.reshape(conv7u, tf.shape(conv7u))\n",
    "    up8 = keras.layers.concatenate([conv7u, conv2], axis=3)\n",
    "    conv8 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='SAME')(up8)\n",
    "    conv8 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='SAME')(conv8)\n",
    "    conv8 = tf.reshape(conv8, tf.shape(conv8))\n",
    "    \n",
    "    conv8u = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=(2, 2), strides=(2, 2), padding='SAME')(conv8)\n",
    "    conv8u = tf.reshape(conv8u, tf.shape(conv8u))\n",
    "    up9 = keras.layers.concatenate([conv8u, conv1], axis=3)\n",
    "    conv9 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='SAME')(up9)\n",
    "    conv9 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='SAME')(conv9)\n",
    "    conv9 = tf.reshape(conv9, tf.shape(conv9))\n",
    "\n",
    "    conv10 = keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "    conv10 = tf.reshape(conv10, tf.shape(conv10))\n",
    "\n",
    "    model = keras.Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    model.compile(optimizer=optimizer(lr=lr), loss=loss_metric, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "# Dice Coefficient to work with Tensorflow\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "# Dice Coefficient to work outside Tensorflow\n",
    "def dice_coef_2(y_true, y_pred):\n",
    "    side = len(y_true[0])\n",
    "    y_true_f = y_true.reshape(side*side)\n",
    "    y_pred_f = y_pred.reshape(side*side)\n",
    "    intersection = sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (sum(y_true_f) + sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_width = 256\n",
    "sample_height = 256\n",
    "\n",
    "model = get_model(optimizer=tf.keras.optimizers.Adam, loss_metric=dice_coef_loss, metrics=[dice_coef], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    453/Unknown - 1994s 4s/step - loss: -0.0098 - dice_coef: 0.0130"
     ]
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "# historyKe = model.fit(train_image_generator, train_mask_generator, epochs=150, batch_size=10)\n",
    "\n",
    "# train_generator = zip(train_image_generator, train_mask_generator)\n",
    "# val_generator = zip(val_image_generator, val_mask_generator)\n",
    "from keras import backend as K\n",
    "\n",
    "historyTF = model.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Model_ep10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "NO_OF_EPOCHS = 30\n",
    "NO_OF_TRAINING_IMAGES = len(os.listdir('./data/train_image/23_train_image/'))\n",
    "NO_OF_VAL_IMAGES = len(os.listdir('./data/val_image/23_test_image/'))\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# checkpoint = ModelCheckpoint(weights_path, monitor='METRIC_TO_MONITOR', \n",
    "#                              verbose=1, save_best_only=True, mode='max')\n",
    "# csv_logger = CSVLogger('./log.out', append=True, separator=';')\n",
    "# earlystopping = EarlyStopping(monitor = 'METRIC_TO_MONITOR', verbose = 1,\n",
    "#                               min_delta = 0.01, patience = 3, mode = 'max')\n",
    "\n",
    "# callbacks_list = [checkpoint, csv_logger, earlystopping]\n",
    "\n",
    "results = model.fit_generator(train_generator, epochs=NO_OF_EPOCHS, \n",
    "                          steps_per_epoch = (NO_OF_TRAINING_IMAGES//BATCH_SIZE),\n",
    "                          validation_data=val_generator, \n",
    "                          validation_steps=(NO_OF_VAL_IMAGES//BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN layer calculate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [filter size, stride, padding]\n",
    "#Assume the two dimensions are the same\n",
    "#Each kernel requires the following parameters:\n",
    "# - k_i: kernel size\n",
    "# - s_i: stride\n",
    "# - p_i: padding (if padding is uneven, right padding will higher than left padding; \"SAME\" option in tensorflow)\n",
    "# \n",
    "#Each layer i requires the following parameters to be fully represented: \n",
    "# - n_i: number of feature (data layer has n_1 = imagesize )\n",
    "# - j_i: distance (projected to image pixel distance) between center of two adjacent features\n",
    "# - r_i: receptive field of a feature in layer i\n",
    "# - start_i: position of the first feature's receptive field in layer i (idx start from 0, negative means the center fall into padding)\n",
    "\n",
    "import math\n",
    "convnet =   [[11,4,0],[3,2,0],[5,1,2],[3,2,0],[3,1,1],[3,1,1],[3,1,1],[3,2,0],[6,1,0], [1, 1, 0]]\n",
    "layer_names = ['conv1','pool1','conv2','pool2','conv3','conv4','conv5','pool5','fc6-conv', 'fc7-conv']\n",
    "imsize = 227\n",
    "\n",
    "def outFromIn(conv, layerIn):\n",
    "  n_in = layerIn[0]\n",
    "  j_in = layerIn[1]\n",
    "  r_in = layerIn[2]\n",
    "  start_in = layerIn[3]\n",
    "  k = conv[0]\n",
    "  s = conv[1]\n",
    "  p = conv[2]\n",
    "  \n",
    "  n_out = math.floor((n_in - k + 2*p)/s) + 1\n",
    "  actualP = (n_out-1)*s - n_in + k \n",
    "  pR = math.ceil(actualP/2)\n",
    "  pL = math.floor(actualP/2)\n",
    "  \n",
    "  j_out = j_in * s\n",
    "  r_out = r_in + (k - 1)*j_in\n",
    "  start_out = start_in + ((k-1)/2 - pL)*j_in\n",
    "  return n_out, j_out, r_out, start_out\n",
    "  \n",
    "def printLayer(layer, layer_name):\n",
    "  print(layer_name + \":\")\n",
    "  print(\"\\t n features: %s \\n \\t jump: %s \\n \\t receptive size: %s \\t start: %s \" % (layer[0], layer[1], layer[2], layer[3]))\n",
    " \n",
    "layerInfos = []\n",
    "if __name__ == '__main__':\n",
    "#first layer is the data layer (image) with n_0 = image size; j_0 = 1; r_0 = 1; and start_0 = 0.5\n",
    "  print (\"-------Net summary------\")\n",
    "  currentLayer = [imsize, 1, 1, 0.5]\n",
    "  printLayer(currentLayer, \"input image\")\n",
    "  for i in range(len(convnet)):\n",
    "    currentLayer = outFromIn(convnet[i], currentLayer)\n",
    "    layerInfos.append(currentLayer)\n",
    "    printLayer(currentLayer, layer_names[i])\n",
    "  print (\"------------------------\")\n",
    "  layer_name = raw_input (\"Layer name where the feature in: \")\n",
    "  layer_idx = layer_names.index(layer_name)\n",
    "  idx_x = int(raw_input (\"index of the feature in x dimension (from 0)\"))\n",
    "  idx_y = int(raw_input (\"index of the feature in y dimension (from 0)\"))\n",
    "  \n",
    "  n = layerInfos[layer_idx][0]\n",
    "  j = layerInfos[layer_idx][1]\n",
    "  r = layerInfos[layer_idx][2]\n",
    "  start = layerInfos[layer_idx][3]\n",
    "  assert(idx_x < n)\n",
    "  assert(idx_y < n)\n",
    "  \n",
    "  print (\"receptive field: (%s, %s)\" % (r, r))\n",
    "  print (\"center: (%s, %s)\" % (start+idx_x*j, start+idx_y*j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
